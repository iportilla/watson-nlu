{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ AI Insights with IBM Watson Natural Language Understanding (NLU)\n",
                "\n",
                "Welcome to the enhanced version of the Watson NLU workshop! This notebook is designed for students and junior developers who want to understand how to turn raw text into actionable insights using AI."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß† Understanding the NLU Pipeline\n",
                "\n",
                "Before we dive into the code, let's look at how data flows through our application:\n",
                "\n",
                "![NLU Workflow](nlu_workflow.png)\n",
                "\n",
                "1.  **Text Data**: We start with customer complaints from a CSV file.\n",
                "2.  **Watson NLU**: This is our \"AI Engine.\" It processes the text and extracts key features.\n",
                "3.  **Insights**: We get back structured data like Sentiment (Positive/Negative) and Emotions (Anger, Joy, etc.).\n",
                "4.  **Analysis**: We use Pandas and Matplotlib to visualize these trends."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.0 Setup - Installing the \"Waiter\"\n",
                "\n",
                "In programming, an **SDK (Software Development Kit)** acts like a waiter in a restaurant. You (the client) give it an order, and it goes to the kitchen (IBM Cloud) and brings back the information you need."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install the necessary libraries\n",
                "!pip install --upgrade ibm-watson ibm-cloud-sdk-core PyJWT python-dotenv pandas matplotlib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üóùÔ∏è Authentication & Security\n",
                "\n",
                "We use the `python-dotenv` library to keep our API keys secret. We store them in a file named `.env` and load them here. \n",
                "\n",
                "> **Student Tip**: Never hardcode your API keys directly in the notebook! If you share the notebook, others could use your credits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from dotenv import load_dotenv\n",
                "from ibm_watson import NaturalLanguageUnderstandingV1\n",
                "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
                "from ibm_watson.natural_language_understanding_v1 import Features, CategoriesOptions, EmotionOptions, KeywordsOptions\n",
                "\n",
                "# Load environment variables and force override to pick up any recent .env changes\n",
                "load_dotenv(override=True)\n",
                "\n",
                "IAM_KEY = os.getenv('IAM_KEY')\n",
                "SERVICE_URL = os.getenv('SERVICE_URL')\n",
                "\n",
                "if not IAM_KEY or not SERVICE_URL:\n",
                "    print(\"‚ùå Error: API Key or Service URL missing. Please check your .env file!\")\n",
                "else:\n",
                "    print(\"‚úÖ Credentials loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.0 Testing the Service\n",
                "\n",
                "Let's make sure everything is working by analyzing a simple URL. We're asking Watson to look at `www.ibm.com` and give us the top 3 **Categories** (what is the site about?)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "authenticator = IAMAuthenticator(IAM_KEY)\n",
                "nlu = NaturalLanguageUnderstandingV1(\n",
                "    version='2022-04-07', # This version date controls the API's behavior rules\n",
                "    authenticator=authenticator\n",
                ")\n",
                "\n",
                "nlu.set_service_url(SERVICE_URL)\n",
                "\n",
                "try:\n",
                "    response = nlu.analyze(\n",
                "        url='https://www.ibm.com',\n",
                "        features=Features(categories=CategoriesOptions(limit=3))\n",
                "    ).get_result()\n",
                "\n",
                "    print(json.dumps(response, indent=2))\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå NLU Analysis failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.0 Working with Real Data\n",
                "\n",
                "We'll load a dataset of consumer complaints about a bank. This is where the real power of NLU shines‚Äîprocessing hundreds of text entries at once."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_url = 'https://raw.githubusercontent.com/IBM/python-and-analytics/master/data/cfpbciti.csv'\n",
                "df = pd.read_csv(data_url)\n",
                "print(f\"Dataset loaded: {df.shape[0]} rows found.\")\n",
                "df.head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.0 Data Cleaning üßπ\n",
                "\n",
                "AI is like a chef: if you give it bad ingredients (noisy data), you'll get a bad result. \n",
                "\n",
                "Our data contains \"XX/XX/XXXX\" placeholders where real names or dates were removed for privacy. We should clean these out to help Watson focus on the important words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Drop rows where there is no text to analyze\n",
                "df_clean = df.dropna(subset=['Consumer complaint narrative'])\n",
                "\n",
                "# 2. Remove the 'X' privacy masks using Regular Expressions (Regex)\n",
                "df_clean = df_clean.replace(regex=['X+'], value='')\n",
                "\n",
                "# 3. Reset the index so our row numbers are sequential (0, 1, 2...)\n",
                "df_clean = df_clean.reset_index(drop=True)\n",
                "\n",
                "print(f\"Cleaned data: {df_clean.shape[0]} rows ready for analysis.\")\n",
                "df_clean['Consumer complaint narrative'].head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.0 Advanced NLU Analysis\n",
                "\n",
                "Now we'll do something cool: we'll take the first 20 complaints and ask Watson to find the **top keywords** AND the **emotions** associated with those keywords."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_to_analyze = 20\n",
                "results = []\n",
                "\n",
                "for i in range(num_to_analyze):\n",
                "    text = df_clean.loc[i, 'Consumer complaint narrative']\n",
                "    \n",
                "    try:\n",
                "        # We analyze Keywords AND Emotions for each complaint\n",
                "        response = nlu.analyze(\n",
                "            text=text,\n",
                "            features=Features(keywords=KeywordsOptions(emotion=True, limit=2))\n",
                "        ).get_result()\n",
                "        \n",
                "        # Extract the highest emotion score for each entry\n",
                "        if response['keywords']:\n",
                "            top_keyword = response['keywords'][0]\n",
                "            results.append({\n",
                "                'id': i,\n",
                "                'keyword': top_keyword['text'],\n",
                "                'anger': top_keyword['emotion']['anger'],\n",
                "                'joy': top_keyword['emotion']['joy'],\n",
                "                'sadness': top_keyword['emotion']['sadness'],\n",
                "                'Product': df_clean.loc[i, 'Product'],\n",
                "                'Sub-product': df_clean.loc[i, 'Sub-product']\n",
                "            })\n",
                "    except Exception as e:\n",
                "        print(f\"Skipping row {i} due to analysis error.\")\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6.0 Data Visualization üìä\n",
                "\n",
                "Finally, we'll plot our findings in a 3D chart. This lets us see if certain products (like Credit Cards) have higher \"Anger\" scores than others."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "\n",
                "fig = plt.figure(figsize=(12, 8))\n",
                "ax = fig.add_subplot(111, projection='3d')\n",
                "\n",
                "# Convert categories to numbers for the axes\n",
                "x_labels, x_indices = np.unique(results_df['Sub-product'], return_inverse=True)\n",
                "y_labels, y_indices = np.unique(results_df['Product'], return_inverse=True)\n",
                "z_data = results_df['anger']\n",
                "\n",
                "sc = ax.scatter(x_indices, y_indices, z_data, c=z_data, cmap='Reds', s=100)\n",
                "\n",
                "ax.set_xticks(range(len(x_labels)))\n",
                "ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
                "ax.set_yticks(range(len(y_labels)))\n",
                "ax.set_yticklabels(y_labels)\n",
                "ax.set_zlabel('Anger Score')\n",
                "ax.set_title('Anger Analysis by Product Category')\n",
                "\n",
                "plt.colorbar(sc, label='Level of Anger')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèÅ Conclusion\n",
                "\n",
                "Congratulations! You've learned how to:\n",
                "1.  Securely authenticate with IBM Watson.\n",
                "2.  Clean text data for AI analysis.\n",
                "3.  Extract Keywords and Emotions from consumer complaints.\n",
                "4.  Visualize emotional trends in 3D.\n",
                "\n",
                "Try changing the `num_to_analyze` variable or adding more `features` like `Entities` or `Sentiment` to explore more!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}